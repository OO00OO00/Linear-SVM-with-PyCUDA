{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\willf\\Anaconda3\\lib\\site-packages\\skcuda\\cublas.py:273: UserWarning: creating CUBLAS context to get version number\n",
      "  warnings.warn('creating CUBLAS context to get version number')\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numba as nb\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import scipy as sp\n",
    "\n",
    "import skcuda\n",
    "import pycuda.autoinit\n",
    "import pycuda.gpuarray as gpuarray\n",
    "import skcuda.linalg as culinalg\n",
    "from pycuda.compiler import SourceModule\n",
    "from pycuda.autoinit import context\n",
    "import pycuda.driver as drv\n",
    "pycuda.autoinit.device\n",
    "culinalg.init()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import itertools\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('https://web.stanford.edu/~hastie/ElemStatLearn/datasets/vowel.train', sep=',', header=0)\n",
    "X_test = pd.read_csv('https://web.stanford.edu/~hastie/ElemStatLearn/datasets/vowel.test', sep=',', header=0)\n",
    "\n",
    "y_train = np.float32(X_train.iloc[:,1]) - 1\n",
    "y_test = np.float32(X_test.iloc[:,1]) - 1\n",
    "X_train = np.float32(X_train.drop(X_train.columns[1], axis=1).as_matrix())\n",
    "X_test = np.float32(X_test.drop(X_test.columns[1], axis=1).as_matrix())\n",
    "\n",
    "# Scale data according to the training data\n",
    "scaler_all = preprocessing.StandardScaler()\n",
    "scaler_all.fit(X_train)\n",
    "X_train = scaler_all.transform(X_train)\n",
    "X_test = scaler_all.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: np.ndarray, x\n",
    "# Output: l2-norm of x\n",
    "# Note: This seems to be marginally faster than np.linalg.norm...\n",
    "def fasterNorm(x):\n",
    "    return np.sqrt(x.dot(x))\n",
    "\n",
    "# Input:\n",
    "# - beta: model coefficients, np.ndarray of dimension (d,)\n",
    "# - lambduh: l2-regularization parameter, float\n",
    "# - y_y_hat: Element-wise product of y & y_hat (i.e., X.dot(beta)), np.ndarray of dimension (n,)\n",
    "# Output: Huberized squared hinge loss objective\n",
    "def obj(beta, lambduh, y_y_hat):\n",
    "    loss = np.mean(svmObjCondition(y_y_hat, 0.5))\n",
    "    return loss + lambduh * fasterNorm(beta)**2\n",
    "\n",
    "# Input:\n",
    "# - x: alias for y_y_hat, i.e., element-wise product of y & y_hat (i.e., X.dot(beta)), np.ndarray of dimension (n,)\n",
    "# - h: threshold value for Huberized squared hinge loss objective, float\n",
    "# Output: Element-wise conditional component of Huberized squared hinge loss objective, np.ndarray of dimension (n,)\n",
    "# Note: Uses numba.vectorize to quickly apply conditions to x\n",
    "@nb.vectorize([nb.float64(nb.float64, nb.float64)])\n",
    "def svmObjCondition(x, h = 0.5):\n",
    "    if 1 - x < -h:\n",
    "        return 0\n",
    "    if 1 - x > h:\n",
    "        return 1 - x\n",
    "    \n",
    "    return (1 - x + h)**2 / (4 * h)\n",
    "\n",
    "# Input:\n",
    "# - x: alias for y_y_hat, i.e., element-wise product of y & y_hat (i.e., X.dot(beta)), np.ndarray of dimension (n,)\n",
    "# - h: threshold value for Huberized squared hinge loss objective, float\n",
    "# Output: Element-wise conditional partial component of gradient of Huberized squared hinge loss objective, np.ndarray of dimension (n,)\n",
    "# Note: Uses numba.vectorize to quickly apply conditions to x\n",
    "@nb.vectorize([nb.float64(nb.float64, nb.float64)])\n",
    "def svmGradCondition(x, h = 0.5):\n",
    "    if 1 - x < -h:\n",
    "        return 0\n",
    "    if 1 - x > h:\n",
    "        return 1\n",
    "    \n",
    "    return (1 - x + h) / (2 * h)\n",
    "\n",
    "# These are custom PyCUDA kernels in C\n",
    "# These do the same purpose as svmGradCondition & svmObjCondition, but applied to pycuda.GPUArray objects\n",
    "modObj = SourceModule(\n",
    "\"\"\"\n",
    "/*\n",
    "* svmObjCondition_GPU is a GPU kernel that applies the element-wise conditional loss function \n",
    "* from the Huberized squared hinge loss.\n",
    "*\n",
    "* See obj_GPU for details.\n",
    "*\n",
    "* Input: a, pycuda.gpuarray.GPUArray (n,1) array\n",
    "* Return: None (void)\n",
    "* Action: the pycuda.gpuarray.GPUArray (n,1) array, dest, is updated according to loss function\n",
    "*\n",
    "*/\n",
    "__global__ void svmObjCondition_GPU(float *a, float *dest)\n",
    "{\n",
    "    const int i = threadIdx.x;\n",
    "    if(1 - a[i] < -0.5){\n",
    "        dest[i] = 0;\n",
    "    } else if(1 - a[i] > 0.5){\n",
    "        dest[i] = 1 - a[i];\n",
    "    } else {\n",
    "        dest[i] = (1 - a[i] + 0.5) * (1 - a[i] + 0.5) / 2;\n",
    "    }\n",
    "}\n",
    "\n",
    "/*\n",
    "* svmGradCondition_GPU is a GPU kernel that applies the element-wise (partial) gradient\n",
    "* of the conditional loss function from the Huberized squared hinge loss. The full gradient\n",
    "* is computed downstream by performing a particular GPU matrix product. \n",
    "*\n",
    "* See computegrad_GPU for details.\n",
    "*\n",
    "* Input: a, pycuda.gpuarray.GPUArray (n,1) array\n",
    "* Return: None (void)\n",
    "* Action: the pycuda.gpuarray.GPUArray (n,1) array, dest, is updated according to partiail gradient\n",
    "*/\n",
    "__global__ void svmGradCondition_GPU(float *a, float *dest)\n",
    "{\n",
    "    const int i = threadIdx.x;\n",
    "    if(1 - a[i] < -0.5){\n",
    "        dest[i] = 0;\n",
    "    } else if(1 - a[i] > 0.5){\n",
    "        dest[i] = 1;\n",
    "    } else {\n",
    "        dest[i] = (1 - a[i] + 0.5);\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# These are prepared call function calls in PyCUDA.\n",
    "# By preparing the function call earlier & declaring expected data types of \"P\" (i.e., GPUArrays),\n",
    "# the functions run much faster.\n",
    "svmObjCondition_GPU = modObj.get_function(\"svmObjCondition_GPU\")\n",
    "svmGradCondition_GPU = modObj.get_function(\"svmGradCondition_GPU\")\n",
    "svmObjCondition_GPU.prepare(\"PP\")\n",
    "svmGradCondition_GPU.prepare(\"PP\")\n",
    "\n",
    "# Input:\n",
    "# - shape: integer tuple representing desired shape of GPUArray\n",
    "# - ary1D: np.ndarray of dimension (n,) containing data to be stored in the GPUArray Data_GPU\n",
    "# - stream: PyCUDA stream for asynchronous data transfer between host and device\n",
    "# Output:\n",
    "# - Data_aligned: memory aligned array of dimension (n,1) containing data from ary1D\n",
    "# - Data_GPU_da: PyCUDA.DeviceAllocation object denoting where data is stored on the GPU\n",
    "# - Data_pin: pinned memory array of dimension (n,1) linked to Data_aligned\n",
    "# - Data_GPU: pycuda.GPUArray object generated by Data_GPU_da and Data_pin\n",
    "# Assume: shape = (n, 1) for some n, to facilitate GPU matrix multiplication requiring such dimensions\n",
    "def pinArrayToGPU(shape, ary1D, stream):\n",
    "    # Align memory\n",
    "    Data_aligned = drv.aligned_empty(shape = shape, dtype = np.float32)\n",
    "    \n",
    "    # Load data\n",
    "    Data_aligned[:,0] = ary1D\n",
    "    \n",
    "    # Allocate GPU memory\n",
    "    Data_GPU_da = drv.mem_alloc(Data_aligned.nbytes)\n",
    "    \n",
    "    # Pin memory\n",
    "    Data_pin = drv.register_host_memory(Data_aligned)\n",
    "    \n",
    "    # Send to GPU\n",
    "    drv.memcpy_htod_async(Data_GPU_da, Data_pin, stream)\n",
    "    \n",
    "    # Create GPUArray object\n",
    "    Data_GPU = gpuarray.GPUArray(shape = shape, dtype = np.float32, gpudata = Data_GPU_da)\n",
    "    \n",
    "    # Wait for kernel to complete before returning to CPU\n",
    "    drv.Context.synchronize()\n",
    "    \n",
    "    return Data_aligned, Data_GPU_da, Data_pin, Data_GPU\n",
    "\n",
    "# Input:\n",
    "# - shape: integer tuple representing desired shape of GPUArray\n",
    "# - ary2D: np.ndarray of dimension (n,d) containing data to be stored in the GPUArray Data_GPU\n",
    "# - stream: PyCUDA stream for asynchronous data transfer between host and device\n",
    "# Output:\n",
    "# - Data_aligned: memory aligned array of dimension (n,d) containing data from ary1D\n",
    "# - Data_GPU_da: PyCUDA.DeviceAllocation object denoting where data is stored on the GPU\n",
    "# - Data_pin: pinned memory array of dimension (n,d) linked to Data_aligned\n",
    "# - Data_GPU: pycuda.GPUArray object generated by Data_GPU_da and Data_pin\n",
    "def pinMatrixToGPU(shape, ary2D, stream):\n",
    "    # Align memory\n",
    "    Data_aligned = drv.aligned_empty(shape = shape, dtype = np.float32)\n",
    "    \n",
    "    # Load data\n",
    "    Data_aligned[:,:] = ary2D\n",
    "    \n",
    "    # Allocate GPU memory\n",
    "    Data_GPU_da = drv.mem_alloc(Data_aligned.nbytes)\n",
    "    \n",
    "    # Pin memory\n",
    "    Data_pin = drv.register_host_memory(Data_aligned)\n",
    "    \n",
    "    # Send to GPU\n",
    "    drv.memcpy_htod_async(Data_GPU_da, Data_pin, stream)\n",
    "    \n",
    "    # Create GPUArray object\n",
    "    Data_GPU = gpuarray.GPUArray(shape = shape, dtype = np.float32, gpudata = Data_GPU_da)\n",
    "    \n",
    "    # Wait for kernel to complete before returning to CPU\n",
    "    drv.Context.synchronize()\n",
    "    \n",
    "    return Data_aligned, Data_GPU_da, Data_pin, Data_GPU\n",
    "\n",
    "# Input:\n",
    "# - shape: integer tuple representing desired shape of GPUArray\n",
    "# - Data_aligned: memory aligned array of dimension (n,d) containing data from ary1D\n",
    "# - Data_GPU_da: PyCUDA.DeviceAllocation object denoting where data is stored on the GPU\n",
    "# - Data_pin: pinned memory array of dimension (n,d) linked to Data_aligned\n",
    "# - ary1D_new: np.ndarray of dimension (n,) containing new data to be stored in the GPUArray Data_GPU\n",
    "# - stream: PyCUDA stream for asynchronous data transfer between host and device\n",
    "# Output:\n",
    "# - Data_GPU: pycuda.GPUArray object generated by Data_GPU_da and Data_pin\n",
    "def updateGPUArray(shape, Data_aligned, Data_GPU_da, Data_pin, ary1D_new, stream):\n",
    "    # Change aligned variable\n",
    "    Data_aligned[:,0] = ary1D_new\n",
    "\n",
    "    # Update the GPU device allocation\n",
    "    drv.memcpy_htod_async(Data_GPU_da, Data_pin, stream)\n",
    "\n",
    "    # Build new GPU object\n",
    "    Data_GPU = gpuarray.GPUArray(shape = shape, dtype = np.float32, gpudata = Data_GPU_da)\n",
    "    \n",
    "    # Wait for kernel to complete before returning to CPU\n",
    "    drv.Context.synchronize()\n",
    "    \n",
    "    return Data_GPU\n",
    "\n",
    "# Input:\n",
    "# - X_GPU: pycuda.GPUArray object representing design matrix X\n",
    "# - beta_GPU: pycuda.GPUArray object representing coefficient array beta\n",
    "# - y_hat_GPU: pycuda.GPUArray object to store GPU-equivalent output of X.dot(beta)\n",
    "# - y_hat_GPU_da: PyCUDA.DeviceAllocation object denoting where data is stored on the GPU for y_hat_GPU\n",
    "# - y_hat_pin: pinned memory array of dimension (n,1) associated with y_hat_GPU\n",
    "# - stream: PyCUDA stream for asynchronous data transfer between host and device\n",
    "# Output:\n",
    "# Void, but y_hat_GPU contains the GPU-equivalent output of X.dot(beta)\n",
    "def compute_y_hat_GPU(X_GPU, beta_GPU, y_hat_GPU_da, y_hat_pin, y_hat_GPU, stream):\n",
    "    # Use scikit-cuda package to perform matrix-vector multiplication on GPU\n",
    "    # Store result in y_hat_GPU\n",
    "    culinalg.dot(X_GPU, beta_GPU, out = y_hat_GPU)\n",
    "    \n",
    "    # Update the host pinned memory object\n",
    "    drv.memcpy_dtoh_async(y_hat_pin, y_hat_GPU_da, stream)\n",
    "    \n",
    "    # Wait for kernel to complete before returning to CPU\n",
    "    drv.Context.synchronize()\n",
    "    \n",
    "# Purpose: Compute objective\n",
    "# Input:\n",
    "# - beta: model coefficients, np.ndarray of dimension (d,)\n",
    "# - lambduh: l2-regularization parameter, float\n",
    "# - y_y_hat_GPU: pycuda.GPUArray object storing GPU-equivalent output of np.multiply(y, y_hat)\n",
    "# - y_y_hat_mod_GPU: dummy pycuda.GPUArray object to store output from svmObjCondition_GPU\n",
    "# - block: tuple of dimension 3 containing block lengths\n",
    "# - grid: tuple of dimension 2 containing grid lengths\n",
    "# - y_y_hat_mod_pin: pinned memory associated with y_y_hat_mod_GPU\n",
    "# - y_y_hat_mod_GPU_da: device allocation objection associated with y_y_hat_mod_GPU\n",
    "# - stream: PyCUDA stream for asynchronous data transfer between host and device\n",
    "# Output: scalar representing Huberized squared hinge loss objective\n",
    "def obj_GPU(beta, lambduh, y_y_hat_GPU, y_y_hat_mod_GPU, block, grid, y_y_hat_mod_pin, y_y_hat_mod_GPU_da, stream):\n",
    "\n",
    "    # Apply conditional loss function of SVM to the product of y & y_hat, using a GPU kernel.\n",
    "    # To improve performance, assume svmObjCondition_GPU has been prepared within pyCUDA, \n",
    "    # and so a prepared_call is performed.\n",
    "    svmObjCondition_GPU.prepared_call(grid, block, y_y_hat_GPU.gpudata, y_y_hat_mod_GPU.gpudata)\n",
    "    \n",
    "    # Peform fast copy of data from GPU to CPU for the output from svmObjCondition_GPU\n",
    "    drv.memcpy_dtoh_async(y_y_hat_mod_pin, y_y_hat_mod_GPU_da, stream)\n",
    "    \n",
    "    # Wait for kernel to complete before returning to CPU\n",
    "    drv.Context.synchronize()\n",
    "\n",
    "    # Return mean loss with l2-regularization component\n",
    "    return np.mean(y_y_hat_mod_pin) + lambduh * np.dot(beta, beta)\n",
    "\n",
    "# Purpose: Compute gradient of objective\n",
    "# Input:\n",
    "# - beta: model coefficients, np.ndarray of dimension (d,)\n",
    "# - lambduh: l2-regularization parameter, float\n",
    "# - n: number of observations in y_y_hat_GPU\n",
    "# - y_y_hat_GPU: pycuda.GPUArray object storing GPU-equivalent output of np.multiply(y, y_hat)\n",
    "# - y_y_hat_mod_GPU: dummy pycuda.GPUArray object to store output from svmGradCondition_GPU\n",
    "# - minusX_row_yT_GPU: pycuda.GPUArray representing -(X * y[:,None]).T\n",
    "# - block: tuple of dimension 3 containing block lengths\n",
    "# - grid: tuple of dimension 2 containing grid lengths\n",
    "# - grad_loss_GPU: pycuda.GPUArray object to store GPU-equivalent output of -(X * y[:,None]).T.dot(y_y_hat_mod)\n",
    "# - grad_loss_GPU_da: PyCUDA.DeviceAllocation object denoting where data is stored on the GPU for grad_loss_GPU\n",
    "# - grad_loss_pin: pinned memory array of dimension (d,1) associated with grad_loss_GPU\n",
    "# - stream: PyCUDA stream for asynchronous data transfer between host and device\n",
    "# Output: np.ndarray(size = d) representing gradient of objective given inputs\n",
    "def computegrad_GPU(beta, lambduh, n, y_y_hat_GPU, y_y_hat_mod_GPU, minusX_row_yT_GPU, block, grid, grad_loss_pin, grad_loss_GPU_da, grad_loss_GPU, stream):\n",
    "    \n",
    "    # Apply (partial) gradient of conditional loss function to the product of y & y_hat, using a GPU kernel.\n",
    "    # To improve performance, assume svmGradCondition_GPU has been prepared within pyCUDA, \n",
    "    # and so a prepared_call is performed.\n",
    "    svmGradCondition_GPU.prepared_call(grid, block, y_y_hat_GPU.gpudata, y_y_hat_mod_GPU.gpudata)\n",
    "    \n",
    "    # Compute product -(X * y[:,None]).T.dot(y_y_hat_mod) with GPU objects\n",
    "    culinalg.dot(minusX_row_yT_GPU, y_y_hat_mod_GPU, out = grad_loss_GPU)\n",
    "    \n",
    "    # Peform fast copy of data from GPU to CPU for the output from svmGradCondition_GPU\n",
    "    drv.memcpy_dtoh_async(grad_loss_pin, grad_loss_GPU_da, stream)\n",
    "    \n",
    "    # Wait for kernel to complete before returning to CPU\n",
    "    drv.Context.synchronize()\n",
    "    \n",
    "    return grad_loss_pin/n + 2 * lambduh * beta\n",
    "\n",
    "# Input:\n",
    "# - x: beta coefficient\n",
    "# - grad_x: gradient of x\n",
    "# - y_y_hat: array equal to np.multiply(y, y_hat), where y_hat = X.dot(beta)\n",
    "# - y_y_hat_grad: array equal to np.multiply(y, y_hat_grad), where y_hat_grad = X.dot(gradient(beta))\n",
    "# - lambduh: l2-regularization parameter, float\n",
    "# - n: deprecated parameter, ignore\n",
    "# - objCurrent: current objective value\n",
    "# - alpha: backtracking parameter used to check condition\n",
    "# - gamma: backtracking parameter used to shrink x as needed\n",
    "# - max_iter: maximum backtracking iterations\n",
    "# Output:\n",
    "def backtracking_GPU(x, grad_x, y_y_hat, y_y_hat_grad, lambduh, n, objCurrent, t = 1, alpha = 0.5, gamma = 0.8, max_iter = 100):\n",
    "    norm_factor = alpha * np.dot(grad_x, grad_x)\n",
    "    found_t = False\n",
    "    i = 0\n",
    "    while (found_t is False and i < max_iter):\n",
    "        beta_test = x - t * grad_x\n",
    "        y_y_hat_test = y_y_hat - t * y_y_hat_grad\n",
    "        if obj(beta = beta_test, lambduh = lambduh, y_y_hat = y_y_hat_test) < objCurrent - t * norm_factor:\n",
    "            found_t = True\n",
    "        elif i == max_iter - 1:\n",
    "            raise('Maximum number of iterations of backtracking reached')\n",
    "        else:\n",
    "            t *= gamma\n",
    "            i += 1\n",
    "    return t\n",
    "\n",
    "# Fast gradient descent algorithm with Huberized squared hinge loss objective\n",
    "# Input:\n",
    "# - beta_init: initial model coefficients, np.ndarray of dimension (d,)\n",
    "# - theta_init: initial model coefficients, np.ndarray of dimension (d,)\n",
    "# - lambduh: l2-regularization parameter, float\n",
    "# - eta_init: initial step size for fast gradient descent algorithm, float\n",
    "# - X: design matrix as np.ndarray with dimension (n,d)\n",
    "# - y: response vector as np.ndarray with dimension (n,)\n",
    "# - eps: Convergence tolerance for norm of gradient of beta\n",
    "# - onlyFinalIteration: parameter designating whether data should only be stored from the first and last iterations\n",
    "# Output: Dictionary containing,\n",
    "# - beta: np.array containing saved beta coefficients from selected iterations\n",
    "# - theta: np.array containing saved theta coefficients from selected iterations\n",
    "# - norm grad beta: np.array containing saved l2-norm of gradient of beta coefficients from selected iterations\n",
    "# - eta: np.array containing saved eta step-sizes from selected iterations\n",
    "# - obj: np.array containing saved objective values from selected iterations\n",
    "# - final iter: number of iterations required before convergence tolerance met\n",
    "def mylinearsvm_GPU(beta_init, theta_init, lambduh, eta_init, X, y, eps = 0.001, onlyFinalIteration = 0):\n",
    "    # Setup & initialization\n",
    "    n, d = X.shape\n",
    "    beta = np.copy(beta_init)\n",
    "    theta = np.copy(theta_init)\n",
    "    X_row_y = X * y[:, None]\n",
    "    minusX_row_yT_GPU = gpuarray.to_gpu(np.ascontiguousarray(-X_row_y.T))\n",
    "    y_hat_beta = np.empty_like(y)\n",
    "    y_hat_theta = np.empty_like(y)\n",
    "    \n",
    "    # Define variables related to pyCUDA prepared_calls for custom kernels\n",
    "    grid = (1, 1)\n",
    "    block = (1024,1,1)\n",
    "    stream = drv.Stream()\n",
    "    \n",
    "    # Create GPU-related objects\n",
    "    X_aligned, X_GPU_da, X_pin, X_GPU = pinMatrixToGPU(shape = (n,d), ary2D = X, stream = stream)\n",
    "    beta_aligned, beta_GPU_da, beta_pin, beta_GPU = pinArrayToGPU(shape = (d,1), ary1D = beta, stream = stream)\n",
    "    theta_aligned, theta_GPU_da, theta_pin, theta_GPU = pinArrayToGPU(shape = (d,1), ary1D = theta, stream = stream)\n",
    "    y_hat_beta_aligned, y_hat_beta_GPU_da, y_hat_beta_pin, y_hat_beta_GPU = pinArrayToGPU(shape = (n,1), ary1D = y_hat_beta, stream = stream)\n",
    "    y_hat_theta_aligned, y_hat_theta_GPU_da, y_hat_theta_pin, y_hat_theta_GPU = pinArrayToGPU(shape = (n,1), ary1D = y_hat_theta, stream = stream)\n",
    "    \n",
    "    compute_y_hat_GPU(X_GPU = X_GPU, beta_GPU = beta_GPU, y_hat_GPU_da = y_hat_beta_GPU_da, y_hat_pin = y_hat_beta_pin, y_hat_GPU = y_hat_beta_GPU, stream = stream)\n",
    "    compute_y_hat_GPU(X_GPU = X_GPU, beta_GPU = theta_GPU, y_hat_GPU_da = y_hat_theta_GPU_da, y_hat_pin = y_hat_theta_pin, y_hat_GPU = y_hat_theta_GPU, stream = stream)\n",
    "    \n",
    "    y_y_hat_beta = np.multiply(y, y_hat_beta_pin[:,0])\n",
    "    y_y_hat_theta = np.multiply(y, y_hat_theta_pin[:,0])\n",
    "\n",
    "    # Create more GPU-related objects\n",
    "    y_y_hat_beta_aligned, y_y_hat_beta_GPU_da, y_y_hat_beta_pin, y_y_hat_beta_GPU = pinArrayToGPU(shape = (n,1), ary1D = y_y_hat_beta, stream = stream)\n",
    "    y_y_hat_theta_aligned, y_y_hat_theta_GPU_da, y_y_hat_theta_pin, y_y_hat_theta_GPU = pinArrayToGPU(shape = (n,1), ary1D = y_y_hat_theta, stream = stream)    \n",
    "    \n",
    "    # Dummy variables for GPU calculations\n",
    "    # These store and transfer intermediate calculations in order to use the fast copying between GPU <-> CPU for pinned memory\n",
    "    y_y_hat_mod = np.empty_like(y)\n",
    "    grad_loss = np.empty_like(beta)\n",
    "    y_y_hat_mod_aligned, y_y_hat_mod_GPU_da, y_y_hat_mod_pin, y_y_hat_mod_GPU = pinArrayToGPU(shape = (n,1), ary1D = y_y_hat_mod, stream = stream)\n",
    "    grad_loss_aligned, grad_loss_GPU_da, grad_loss_pin, grad_loss_GPU = pinArrayToGPU(shape = (d,1), ary1D = grad_loss, stream = stream)        \n",
    "    \n",
    "    # Initialize gradients for beta and theta\n",
    "    grad_beta = computegrad_GPU(beta = beta_pin, lambduh = lambduh, n = n, y_y_hat_GPU = y_y_hat_beta_GPU, y_y_hat_mod_GPU = y_y_hat_mod_GPU, minusX_row_yT_GPU = minusX_row_yT_GPU, block = block, grid = grid, grad_loss_pin = grad_loss_pin, grad_loss_GPU_da = grad_loss_GPU_da, grad_loss_GPU = grad_loss_GPU, stream = stream)\n",
    "    grad_theta = computegrad_GPU(beta = theta_pin, lambduh = lambduh, n = n, y_y_hat_GPU = y_y_hat_theta_GPU, y_y_hat_mod_GPU = y_y_hat_mod_GPU, minusX_row_yT_GPU = minusX_row_yT_GPU, block = block, grid = grid, grad_loss_pin = grad_loss_pin, grad_loss_GPU_da = grad_loss_GPU_da, grad_loss_GPU = grad_loss_GPU, stream = stream)\n",
    "    \n",
    "    # Initialize objective values for beta and theta\n",
    "    # Note: leaving syntax for obj_GPU as an example\n",
    "#     obj_current_beta = obj_GPU(beta = beta_pin[:,0], lambduh = lambduh, y_y_hat_GPU = y_y_hat_beta_GPU, y_y_hat_mod_GPU = y_y_hat_mod_GPU, block = block, grid = grid, y_y_hat_mod_pin = y_y_hat_mod_pin, y_y_hat_mod_GPU_da = y_y_hat_mod_GPU_da, stream = stream)\n",
    "#     obj_current_theta = obj_GPU(beta = theta_pin[:,0], lambduh = lambduh, y_y_hat_GPU = y_y_hat_theta_GPU, y_y_hat_mod_GPU = y_y_hat_mod_GPU, block = block, grid = grid, y_y_hat_mod_pin = y_y_hat_mod_pin, y_y_hat_mod_GPU_da = y_y_hat_mod_GPU_da, stream = stream)\n",
    "    obj_current_beta = obj(beta = beta_pin[:,0], lambduh = lambduh, y_y_hat = y_y_hat_beta)\n",
    "    obj_current_theta = obj(beta = theta_pin[:,0], lambduh = lambduh, y_y_hat = y_y_hat_theta)\n",
    "\n",
    "    # Create more GPU-related objects\n",
    "    y_hat_grad_theta = np.empty_like(y)\n",
    "    y_hat_grad_theta_aligned, y_hat_grad_theta_GPU_da, y_hat_grad_theta_pin, y_hat_grad_theta_GPU = pinArrayToGPU(shape = (n,1), ary1D = y_hat_grad_theta, stream = stream)\n",
    "    grad_theta_aligned, grad_theta_GPU_da, grad_theta_pin, grad_theta_GPU = pinArrayToGPU(shape = (d,1), ary1D = grad_theta[:,0], stream = stream)    \n",
    "    \n",
    "    compute_y_hat_GPU(X_GPU = X_GPU, beta_GPU = grad_theta_GPU, y_hat_GPU_da = y_hat_grad_theta_GPU_da, y_hat_pin = y_hat_grad_theta_pin, y_hat_GPU = y_hat_grad_theta_GPU, stream = stream)\n",
    "    y_y_hat_grad_theta = np.multiply(y, y_hat_grad_theta_pin[:,0])\n",
    "    \n",
    "    # Store values\n",
    "    beta_vals = [np.copy(beta)]\n",
    "    theta_vals = [np.copy(theta)]\n",
    "    norm_grad_beta_vals = [np.copy(fasterNorm(grad_beta[:,0]))]\n",
    "    obj_vals = [np.copy(obj_current_beta)]\n",
    "    eta = np.copy(eta_init)\n",
    "    eta_vals = [np.copy(eta)]\n",
    "    iter = 0\n",
    "    while fasterNorm(grad_beta[:,0]) > eps:\n",
    "        eta = backtracking_GPU(x = theta_pin[:,0], grad_x = grad_theta_pin[:,0], y_y_hat = y_y_hat_theta_pin[:,0], y_y_hat_grad = y_y_hat_grad_theta, lambduh = lambduh, n = n, objCurrent = obj_current_theta, t = eta, alpha = 0.5, gamma = 0.8, max_iter = 200)\n",
    "        beta_prior = np.copy(beta)\n",
    "        beta = theta - eta * grad_theta[:,0]\n",
    "        theta = beta + iter / (iter + 3) * (beta - beta_prior)\n",
    "        \n",
    "        # Update GPU beta & theta values\n",
    "        beta_GPU = updateGPUArray(shape = (d,1), Data_aligned = beta_aligned, Data_GPU_da = beta_GPU_da, Data_pin = beta_pin, ary1D_new = np.float32(beta), stream = stream)\n",
    "        theta_GPU = updateGPUArray(shape = (d,1), Data_aligned = theta_aligned, Data_GPU_da = theta_GPU_da, Data_pin = theta_pin, ary1D_new = np.float32(theta), stream = stream)\n",
    "        \n",
    "        # Compute current y_hat variables in GPU\n",
    "        compute_y_hat_GPU(X_GPU = X_GPU, beta_GPU = beta_GPU, y_hat_GPU_da = y_hat_beta_GPU_da, y_hat_pin = y_hat_beta_pin, y_hat_GPU = y_hat_beta_GPU, stream = stream)\n",
    "        compute_y_hat_GPU(X_GPU = X_GPU, beta_GPU = theta_GPU, y_hat_GPU_da = y_hat_theta_GPU_da, y_hat_pin = y_hat_theta_pin, y_hat_GPU = y_hat_theta_GPU, stream = stream)\n",
    "\n",
    "        # Compute current products of y & y_hat in CPU\n",
    "        y_y_hat_beta = np.multiply(y, y_hat_beta_pin[:,0])\n",
    "        y_y_hat_theta = np.multiply(y, y_hat_theta_pin[:,0])\n",
    "        \n",
    "        # Update current products of y & y_hat in GPU\n",
    "        y_y_hat_beta_GPU = updateGPUArray(shape = (n,1), Data_aligned = y_y_hat_beta_aligned, Data_GPU_da = y_y_hat_beta_GPU_da, Data_pin = y_y_hat_beta_pin, ary1D_new = np.float32(y_y_hat_beta), stream = stream)\n",
    "        y_y_hat_theta_GPU = updateGPUArray(shape = (n,1), Data_aligned = y_y_hat_theta_aligned, Data_GPU_da = y_y_hat_theta_GPU_da, Data_pin = y_y_hat_theta_pin, ary1D_new = np.float32(y_y_hat_theta), stream = stream)\n",
    "\n",
    "        # Compute current objective values\n",
    "#         obj_current_beta = obj_GPU(beta = beta_pin[:,0], lambduh = lambduh, y_y_hat_GPU = y_y_hat_beta_GPU, y_y_hat_mod_GPU = y_y_hat_mod_GPU, block = block, grid = grid, y_y_hat_mod_pin = y_y_hat_mod_pin, y_y_hat_mod_GPU_da = y_y_hat_mod_GPU_da, stream = stream)\n",
    "#         obj_current_theta = obj_GPU(beta = theta_pin[:,0], lambduh = lambduh, y_y_hat_GPU = y_y_hat_theta_GPU, y_y_hat_mod_GPU = y_y_hat_mod_GPU, block = block, grid = grid, y_y_hat_mod_pin = y_y_hat_mod_pin, y_y_hat_mod_GPU_da = y_y_hat_mod_GPU_da, stream = stream)\n",
    "        obj_current_beta = obj(beta = beta_pin[:,0], lambduh = lambduh, y_y_hat = y_y_hat_beta)\n",
    "        obj_current_theta = obj(beta = theta_pin[:,0], lambduh = lambduh, y_y_hat = y_y_hat_theta)\n",
    "        \n",
    "        # Compute current gradients in GPU\n",
    "        grad_beta = computegrad_GPU(beta = beta_pin, lambduh = lambduh, n = n, y_y_hat_GPU = y_y_hat_beta_GPU, y_y_hat_mod_GPU = y_y_hat_mod_GPU, minusX_row_yT_GPU = minusX_row_yT_GPU, block = block, grid = grid, grad_loss_pin = grad_loss_pin, grad_loss_GPU_da = grad_loss_GPU_da, grad_loss_GPU = grad_loss_GPU, stream = stream)\n",
    "        grad_theta = computegrad_GPU(beta = theta_pin, lambduh = lambduh, n = n, y_y_hat_GPU = y_y_hat_theta_GPU, y_y_hat_mod_GPU = y_y_hat_mod_GPU, minusX_row_yT_GPU = minusX_row_yT_GPU, block = block, grid = grid, grad_loss_pin = grad_loss_pin, grad_loss_GPU_da = grad_loss_GPU_da, grad_loss_GPU = grad_loss_GPU, stream = stream)\n",
    "        \n",
    "        # Update GPU grad_theta values\n",
    "        grad_theta_GPU = updateGPUArray(shape = (d,1), Data_aligned = grad_theta_aligned, Data_GPU_da = grad_theta_GPU_da, Data_pin = grad_theta_pin, ary1D_new = np.float32(grad_theta[:,0]), stream = stream)\n",
    "        \n",
    "        # Compute current y_hat_grad_theta values\n",
    "        compute_y_hat_GPU(X_GPU = X_GPU, beta_GPU = grad_theta_GPU, y_hat_GPU_da = y_hat_grad_theta_GPU_da, y_hat_pin = y_hat_grad_theta_pin, y_hat_GPU = y_hat_grad_theta_GPU, stream = stream)\n",
    "        \n",
    "        # Compute current products of y & y_hat in CPU\n",
    "        y_y_hat_grad_theta = np.multiply(y, y_hat_grad_theta_pin[:,0])\n",
    "        \n",
    "        iter += 1\n",
    "        # Store values according to onlyFinalIteration\n",
    "        if onlyFinalIteration == 0:\n",
    "            beta_vals.append(np.copy(beta))\n",
    "            theta_vals.append(np.copy(theta))\n",
    "            norm_grad_beta_vals.append(np.copy(fasterNorm(grad_beta[:,0])))\n",
    "            eta_vals.append(np.copy(eta))\n",
    "            obj_vals.append(np.copy(obj_current_beta))\n",
    "    \n",
    "    # Store values according to onlyFinalIteration\n",
    "    if onlyFinalIteration == 1:\n",
    "        beta_vals.append(np.copy(beta))\n",
    "        theta_vals.append(np.copy(theta))\n",
    "        norm_grad_beta_vals.append(np.copy(fasterNorm(grad_beta[:,0])))\n",
    "        eta_vals.append(np.copy(eta))\n",
    "        obj_vals.append(np.copy(obj_current_beta))\n",
    "    \n",
    "    # Manually release device allocation memory\n",
    "    # Note: only added to troubleshoot excessive memory...doesn't seem to work....\n",
    "    X_GPU_da.free()\n",
    "    beta_GPU_da.free()\n",
    "    theta_GPU_da.free()\n",
    "    y_hat_beta_GPU_da.free()\n",
    "    y_hat_theta_GPU_da.free()\n",
    "    y_y_hat_beta_GPU_da.free()\n",
    "    y_y_hat_theta_GPU_da.free()\n",
    "    grad_loss_GPU_da.free()\n",
    "    y_y_hat_mod_GPU_da.free()\n",
    "    grad_theta_GPU_da.free()\n",
    "    y_hat_grad_theta_GPU_da.free()\n",
    "    \n",
    "    # Manually release pinned memory\n",
    "    # Note: only added to troubleshoot excessive memory...doesn't seem to work....\n",
    "    X_pin.base.unregister()\n",
    "    beta_pin.base.unregister()\n",
    "    theta_pin.base.unregister()\n",
    "    y_hat_beta_pin.base.unregister()\n",
    "    y_hat_theta_pin.base.unregister()\n",
    "    y_y_hat_beta_pin.base.unregister()\n",
    "    y_y_hat_theta_pin.base.unregister()\n",
    "    grad_loss_pin.base.unregister()\n",
    "    y_y_hat_mod_pin.base.unregister()\n",
    "    grad_theta_pin.base.unregister()\n",
    "    y_hat_grad_theta_pin.base.unregister()        \n",
    "    \n",
    "    # Delete GPU-related objects\n",
    "    del minusX_row_yT_GPU,X_GPU,beta_GPU,theta_GPU,y_hat_beta_GPU,y_hat_theta_GPU,y_y_hat_beta_GPU,y_y_hat_theta_GPU,grad_loss_GPU,y_y_hat_mod_GPU,grad_theta_GPU,y_hat_grad_theta_GPU\n",
    "    del X_pin,beta_pin,theta_pin,y_hat_beta_pin,y_hat_theta_pin,y_y_hat_beta_pin,y_y_hat_theta_pin,grad_loss_pin,y_y_hat_mod_pin,grad_theta_pin,y_hat_grad_theta_pin\n",
    "    del X_aligned,beta_aligned,theta_aligned,y_hat_beta_aligned,y_hat_theta_aligned,y_y_hat_beta_aligned,y_y_hat_theta_aligned,grad_loss_aligned,y_y_hat_mod_aligned,grad_theta_aligned,y_hat_grad_theta_aligned\n",
    "    \n",
    "    # Run garbage collection\n",
    "    gc.collect()\n",
    "        \n",
    "    return {\"beta\": np.array(beta_vals), \"theta\": np.array(theta_vals), \"norm grad beta\": np.array(norm_grad_beta_vals), \"eta\": np.array(eta_vals), \"obj\": np.array(obj_vals), \"final iter\": iter}\n",
    "\n",
    "# Input:\n",
    "# - lambduh_list: list of l2-regularization parameters to sequentially apply to the label-pair one-vs-one models set by label_combos below, floats\n",
    "# - X: design matrix as np.ndarray with dimension (n,d)\n",
    "# - y: response vector as np.ndarray with dimension (n,)\n",
    "# - k_folds: number of cross-validation folds to perform, integer\n",
    "# - eps: Convergence tolerance for norm of gradient of beta\n",
    "# - eta_init_constant: optional initial step-size eta to be applied to all cross-validation runs.\n",
    "#   If eta_init_constant = None, the initial step-size is inferred via 1 / (max eigenvalue / n + lambduh)\n",
    "# Output: Dictionary containing,\n",
    "# - misclassify means: np.ndarray of dimension (len(lambduh_list), k_folds)\n",
    "# - lambda: copy of input lambduh_list\n",
    "def mylinearsvm_CV_GPU(lambduh_list, X, y, k_folds, eps = 0.001, eta_init_constant = None):\n",
    "    misclassify_vals = []\n",
    "    timing = []\n",
    "    num_labels = len(np.unique(y))\n",
    "    label_combos = list(itertools.combinations(range(num_labels), 2))\n",
    "    num_label_combos = len(label_combos)\n",
    "    \n",
    "    for pairIter in range(num_label_combos):\n",
    "        if pairIter == 0:\n",
    "            avg_run_time = 0\n",
    "            sd_run_time = 0\n",
    "        else:\n",
    "            avg_run_time = np.mean(timing)\n",
    "            sd_run_time = np.std(timing)\n",
    "        print('Working on pair ' + str(pairIter + 1) + ' of ' + str(num_label_combos) + ', Average completion time per CV = ' + str(np.floor(avg_run_time * 100)/100) + ' +/- ' + str(np.floor(sd_run_time*10)/10) + ' s' + ', Expected time until completion = ' + str(np.floor((num_label_combos - pairIter - 1) * avg_run_time)) + ' s', flush = True, end = '\\r')\n",
    "        minLabel = label_combos[pairIter][0]\n",
    "        maxLabel = label_combos[pairIter][1]\n",
    "        X_pair, y_pair = construct_OvO_data(label_pair = [minLabel,maxLabel], X = X, y = y)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        output_CV = mylinearsvm_CV_GPU_backend(lambduh_list = lambduh_list, X = X_pair, y = y_pair, k_folds = k_folds, eps = eps, eta_init = eta_init_constant)\n",
    "        end_time = time.time()\n",
    "        timing.append(end_time - start_time)\n",
    "        \n",
    "        misclassify_vals.append(list(output_CV[\"misclassify means\"]))\n",
    "        del X_pair, y_pair, output_CV\n",
    "        \n",
    "        # Run garbage collection\n",
    "        gc.collect()\n",
    "        \n",
    "    return {\"misclassify means\": misclassify_vals, \"lambda\": np.array(lambduh_list)}\n",
    "\n",
    "# Input:\n",
    "# - lambduh_list: list of l2-regularization parameters to sequentially apply to the label-pair one-vs-one models set by label_combos below, floats\n",
    "# - X: design matrix as np.ndarray with dimension (n,d)\n",
    "# - y: response vector as np.ndarray with dimension (n,)\n",
    "# - k_folds: number of cross-validation folds to perform, integer\n",
    "# - eps: Convergence tolerance for norm of gradient of beta\n",
    "# - eta_init: initial step-size value, eta\n",
    "# Output: Dictionary containing,\n",
    "# - misclassify means: np.ndarray containing means of misclassifications from cross-validation\n",
    "# - lambda: copy of input lambduh_list\n",
    "# Assumes: lambduh_list is ordered from smallest to largest\n",
    "def mylinearsvm_CV_GPU_backend(lambduh_list, X, y, k_folds, eps = 0.001, eta_init = None):\n",
    "    n, d = X.shape\n",
    "    indices_CV = getIndicesCV(n, k_folds)\n",
    "    misclassify_vals = np.empty([len(lambduh_list), k_folds])\n",
    "    \n",
    "    # Do an initial run to get better initial beta guess for CV\n",
    "    beta_init = np.float32(np.random.normal(size=d))\n",
    "    if eta_init is None:\n",
    "        max_eigenvalue = sp.linalg.eigh(np.dot(X.T, X), eigvals_only=True, eigvals=(d-1, d-1))\n",
    "        eta_init = np.float64(1 / (max_eigenvalue / n + lambduh_list[0]))\n",
    "    output_init = mylinearsvm_GPU(beta_init = beta_init, theta_init = np.copy(beta_init), lambduh = np.float32(lambduh_list[0]), eta_init = np.float32(eta_init), X = np.float32(X), y = np.float32(y), eps = eps, onlyFinalIteration = 1)\n",
    "    beta_init = np.float32(np.copy(output_init['beta'][-1]))\n",
    "    theta_init = np.float32(np.copy(output_init['theta'][-1]))\n",
    "    eta_init = np.float32(np.copy(output_init['eta'][-1]))\n",
    "    del output_init\n",
    "    for k in range(k_folds):\n",
    "        X_train_lamb = X[indices_CV[k][1],:]\n",
    "        X_test_lamb = X[indices_CV[k][0],:]\n",
    "        y_train_lamb = y[indices_CV[k][1]]\n",
    "        y_test_lamb = y[indices_CV[k][0]]\n",
    "        n = X_train_lamb.shape[0]\n",
    "\n",
    "        # Scaling\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        scaler.fit(X_train_lamb)\n",
    "        X_train_lamb = scaler.transform(X_train_lamb)\n",
    "        X_test_lamb = scaler.transform(X_test_lamb)\n",
    "        del scaler\n",
    "        \n",
    "        for lamb in range(len(lambduh_list)):\n",
    "            output = mylinearsvm_GPU(beta_init = beta_init, theta_init = theta_init, lambduh = lambduh_list[lamb], eta_init = eta_init, X = np.float32(X_train_lamb), y = np.float32(y_train_lamb), eps = eps, onlyFinalIteration = 1)\n",
    "            beta_init = np.float32(np.copy(output['beta'][-1]))\n",
    "            theta_init = np.float32(np.copy(output['theta'][-1]))\n",
    "            eta_init = np.float32(np.copy(output['eta'][-1]))\n",
    "            \n",
    "            misclassify_vals[lamb, k] = calcMisclassification(output['beta'][-1], X_test_lamb, y_test_lamb)\n",
    "            gc.collect()\n",
    "        \n",
    "        del X_train_lamb, X_test_lamb, y_train_lamb, y_test_lamb, output\n",
    "        \n",
    "        # Run garbage collection\n",
    "        gc.collect()\n",
    "    \n",
    "    misclassify_vals = np.mean(misclassify_vals, axis = 1)\n",
    "        \n",
    "    return {\"misclassify means\": misclassify_vals, \"lambda\": np.array(lambduh_list)}\n",
    "\n",
    "# Input:\n",
    "# - label_pair: tuple of dimension 2 specifying which response categories should be used to construct the subsetted data\n",
    "# Note: label_pair assumes the first element is the smallest, second is largest\n",
    "# - X: design matrix as np.ndarray with dimension (n,d)\n",
    "# - y: response vector as np.ndarray with dimension (n,)\n",
    "# - scale: optional parameter specifying whether the subsetted design matrix X_pair should be scaled\n",
    "# Output:\n",
    "# - X_pair: subset of X associated with records of y matching elements of label_pair\n",
    "# - y_pair: subset of y associated with records of y matching elements of label_pair, where min(label_pair) is mapped to -1 and +1 otherwise.\n",
    "def construct_OvO_data(label_pair, X, y, scale = True):\n",
    "    \n",
    "    # Find relevant category indices with mask\n",
    "    indexes_pair = np.where(np.logical_or(y == label_pair[0], y == label_pair[1]))\n",
    "    y_pair = np.empty_like(y[indexes_pair])\n",
    "    X_pair = X[indexes_pair]\n",
    "\n",
    "    # Define y_pair as vector of -1 and +1 according to record location of label pair in y\n",
    "    y_pair[y[indexes_pair] == label_pair[0]] = -1\n",
    "    y_pair[y[indexes_pair] == label_pair[1]] = 1\n",
    "\n",
    "    # Scale design matrix\n",
    "    if scale:\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        scaler.fit(X_pair)\n",
    "        X_pair = scaler.transform(X_pair)\n",
    "        del scaler\n",
    "    \n",
    "    X_pair = np.float32(X_pair)\n",
    "    y_pair = np.float32(y_pair)\n",
    "        \n",
    "    return X_pair, y_pair\n",
    "\n",
    "# Input:\n",
    "# - lambduh_list: list of l2-regularization parameters to sequentially apply to the label-pair one-vs-one models set by label_combos below, floats\n",
    "# - label_combos: list of 2D tuples representing all possible one-vs-one models with the first element representing the smallest category of the model pair\n",
    "# - beta_init_list: list of initial beta coefficients to use in one-vs-one fitting\n",
    "# - X: design matrix as np.ndarray with dimension (n,d)\n",
    "# - y: response vector as np.ndarray with dimension (n,)\n",
    "# - eps: Convergence tolerance for norm of gradient of beta\n",
    "# - verbose: optional parameter to print additional information about the fitting process for each label pair\n",
    "# Output: Dictionary containing,\n",
    "# - svm output: list of all output from mylinearsvm_GPU for each label pair\n",
    "# - lambda: copy of input lambduh_list\n",
    "# - label combos: copy of input label_combos\n",
    "def mysvm_OvO_GPU(lambduh_list, label_combos, beta_init_list, X, y, eps = 0.0001, verbose = False):\n",
    "#     For a response with 100 categories, label_combos could be:\n",
    "#     label_combos = list(itertools.combinations(range(100), 2))\n",
    "    svm_output_list = []\n",
    "    \n",
    "    for pairIter in range(len(label_combos)):\n",
    "        if verbose:\n",
    "            print('\\nStarting pair ',pairIter,': ',label_combos[pairIter], flush=True)\n",
    "        \n",
    "        # Construct needed data pairs\n",
    "        start_time = time.time()       \n",
    "        minLabel = list(label_combos[pairIter])[0]\n",
    "        maxLabel = list(label_combos[pairIter])[1]\n",
    "        X_pair, y_pair = construct_OvO_data(label_pair = [minLabel,maxLabel], X = X, y = y)\n",
    "        \n",
    "        # Setup initial values\n",
    "        n, d = X_pair.shape\n",
    "        beta_init = np.float32(beta_init_list[pairIter])\n",
    "        max_eigen = sp.linalg.eigh(np.dot(X_pair.T, X_pair), eigvals_only=True, eigvals=(d-1, d-1))\n",
    "        eta_init = np.float64(1 / (max_eigen / n + lambduh_list[pairIter]))\n",
    "        \n",
    "        end_time = time.time()\n",
    "        if verbose:\n",
    "            print(\"Total prep time: {}\".format(end_time - start_time), flush=True)\n",
    "        \n",
    "        # Fit model\n",
    "        start_time = time.time()\n",
    "        output = mylinearsvm_GPU(beta_init = beta_init, theta_init = np.copy(beta_init), lambduh = lambduh_list[pairIter], eta_init = np.float32(eta_init), X = X_pair, y = y_pair, eps = eps, onlyFinalIteration = 1)\n",
    "        svm_output_list.append(output)\n",
    "        end_time = time.time()\n",
    "        if verbose:\n",
    "            print(\"Total fit time: {}\".format(end_time - start_time), flush=True)\n",
    "            \n",
    "    return {\"svm output\": svm_output_list, \"lambda\": np.array(lambduh_list), \"label combos\": np.array(label_combos)}\n",
    "\n",
    "# Input:\n",
    "# - beta: model coefficients, np.ndarray of dimension (d,)\n",
    "# - X: design matrix as np.ndarray with dimension (n,d)\n",
    "# - y: response vector as np.ndarray with dimension (n,)\n",
    "# Output: mean misclassification error between y and y_hat (after mapping positive y_hat values to +1, else -1)\n",
    "def calcMisclassification(beta, X, y):    \n",
    "    y_pred = X.dot(beta) > 0\n",
    "    y_pred = np.int32(2 * y_pred - 1)\n",
    "    return np.mean(y_pred != y)\n",
    "\n",
    "# Input:\n",
    "# - y: response vector as np.ndarray with dimension (n,)\n",
    "# - y_hat: predicted categories as np.ndarray with dimension (n,)\n",
    "# Output: mean misclassification error between y and y_hat\n",
    "def calcMisclassification_allPairs(y, y_hat):\n",
    "    return np.mean(y != y_hat)\n",
    "\n",
    "# Get indices to build cross-validation data subsets\n",
    "# Input:\n",
    "# - n: number of observations in X and y\n",
    "# - k_folds: number of folds to use in cross-validation\n",
    "# Output: list containing training and testing indices for k-folds\n",
    "# E.g., X[indices_CV[i][1],:] represents the training subset of the ith fold\n",
    "# E.g., X[indices_CV[i][0],:] represents the testing subset of the ith fold\n",
    "def getIndicesCV(n, k_folds):\n",
    "    random_index_order = np.random.permutation(range(n))\n",
    "    CV_index_dict = dict(enumerate(np.array_split(random_index_order, k_folds)))\n",
    "    \n",
    "    indices_CV = []\n",
    "    for fold in range(k_folds):\n",
    "        indices_fold = list(range(k_folds))\n",
    "        del indices_fold[fold]\n",
    "        indices_fold_test = CV_index_dict.get(fold)\n",
    "        indices_fold_train = [CV_index_dict.get(key) for key in indices_fold]\n",
    "        indices_fold_train = np.concatenate(indices_fold_train).ravel()\n",
    "        indices_fold = [indices_fold_test, indices_fold_train]\n",
    "        indices_CV.append(list(indices_fold))\n",
    "    return indices_CV\n",
    "\n",
    "# Find mode in vector. If there are multiple modes, randomly choose 1 to return.\n",
    "# Input:\n",
    "# - x: np.ndarray\n",
    "# Output: index with largest argument, if one argmax. otherwise, randomly select indices in {argmax}\n",
    "def modeRandomChoice(x):\n",
    "    labelCounts = np.bincount(x)\n",
    "    modeIndices = np.where(labelCounts == labelCounts.max())\n",
    "    return np.random.choice(np.arange(np.max(x)+1)[modeIndices], 1)\n",
    "\n",
    "# Input:\n",
    "# - X_train: design matrix X used to train models associated with beta_list, np.ndarray of dimension (n,d)\n",
    "# - y_train: response vector y used to train models associated with beta_list, np.ndarray of dimension (n,)\n",
    "# - X_test: new design matrix X to use in scoring, np.ndarray of dimension (n,d)\n",
    "# - beta_list: list of beta coefficients to use in scoring a dataset\n",
    "# - OneVsOneLabelMatrix: np.ndarray representing all possible label pairs, i.e., same as label_combos above\n",
    "# Output: predicted categories from one-vs-one fitting, consistent with beta_list\n",
    "def scoreSVM_OvO(X_train, y_train, X_test, beta_list, OneVsOneLabelMatrix):\n",
    "    n_pairs = OneVsOneLabelMatrix.shape[0]\n",
    "    n = X_test.shape[0]\n",
    "    y_hat_cols = []\n",
    "    \n",
    "    for pairIter in range(n_pairs):        \n",
    "        # Construct needed data pairs   \n",
    "        minLabel = OneVsOneLabelMatrix[pairIter,0]\n",
    "        maxLabel = OneVsOneLabelMatrix[pairIter,1]\n",
    "        X_train_pair, y_train_pair = construct_OvO_data(label_pair = [minLabel,maxLabel], X = X_train, y = y_train, scale = False)\n",
    "        \n",
    "        # Center data-to-score consistent with training data\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        scaler.fit(X_train_pair)\n",
    "        X_train_pair = scaler.transform(X_train_pair)\n",
    "        X_test_pair = scaler.transform(X_test)\n",
    "        \n",
    "        # Score test data\n",
    "        beta_pair = beta_list[pairIter]\n",
    "        y_hat_cols.append(np.int32(X_test_pair.dot(beta_pair) > 0))\n",
    "        \n",
    "    # Restate y_hat_cols as a n x n_pairs array\n",
    "    y_hat_cols = np.vstack(y_hat_cols).T\n",
    "    \n",
    "    # This is an n x n_pairs array\n",
    "    y_hat_rows = np.asarray([range(n_pairs) for i in range(n)])\n",
    "    \n",
    "    # Extract categorical labels\n",
    "    y_hat = OneVsOneLabelMatrix[y_hat_rows, y_hat_cols]\n",
    "    \n",
    "    return np.apply_along_axis(modeRandomChoice, axis = 1, arr = y_hat)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct 1 label pair for model comparison\n",
    "X_pair, y_pair = construct_OvO_data(label_pair = [0,1], X = X_train, y = y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.005208333333333333, class_weight=None, dual=True,\n",
       "     fit_intercept=False, intercept_scaling=1, loss='squared_hinge',\n",
       "     max_iter=1000, multi_class='ovr', penalty='l2', random_state=None,\n",
       "     tol=0.001, verbose=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: This setup uses squared hinge loss, but not the Huberized version. I think SGDClassifier should be used here instead...",
    "skSVM = LinearSVC(penalty = 'l2', fit_intercept = False, C = 1 / (2 * X_pair.shape[0]), tol = 0.001)\n",
    "skSVM.fit(X_pair, y_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MySVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, d = X_pair.shape\n",
    "beta_init = np.float32(np.random.normal(size=d))\n",
    "\n",
    "max_eigenvalue = sp.linalg.eigh(np.dot(X_pair.T, X_pair), eigvals_only=True, eigvals=(d-1, d-1))\n",
    "eta_init = np.float64(1 / (max_eigenvalue / n + 1))\n",
    "\n",
    "mysvm_fit = mylinearsvm_GPU(beta_init = beta_init, theta_init = np.copy(beta_init), lambduh = np.float32(1), eta_init = np.float32(eta_init), X = X_pair, y = y_pair, eps = 0.001, onlyFinalIteration = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit misclassification 0.19791666666666663\n",
      "MySVM misclassification 0.23958333333333334\n",
      "Scikit fitted beta:  [-0.02551857  0.1390112   0.15536878 -0.0522977  -0.17512594]\n",
      "MySVM fitted beta:  [ 0.00281629  0.12550992  0.13767283 -0.05717549 -0.14766912]\n"
     ]
    }
   ],
   "source": [
    "print('Scikit misclassification',1 - skSVM.score(X_pair, y_pair))\n",
    "print('MySVM misclassification',calcMisclassification(mysvm_fit['beta'][-1], X_pair, y_pair))\n",
    "\n",
    "print('Scikit fitted beta: ',skSVM.coef_[0][0:5])\n",
    "print('MySVM fitted beta: ',mysvm_fit['beta'][-1][0:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
